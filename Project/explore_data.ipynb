{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "{'German:\\n': 'SOURCE ', 'English:\\n': 'TARGET '}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "default_ip = '!?.'\n",
    "default_st_tagging = {\"German:\\n\": \"SOURCE \", \"English:\\n\": \"SPLIT \"}\n",
    "default_rm_tagging = {\"Roots in English: \": \"SPLIT \", \"Modifiers in English: \": \"SPLIT \"}\n",
    "\n",
    "def clean_text(file_path, format=\"labeled\", ignored_punctuation=default_ip, source_target_tagging=default_st_tagging, root_modifier_tagging=default_rm_tagging):\n",
    "    \"\"\"\n",
    "    Assumes strict order of German than English\n",
    "    Returns two lists, source and target each of which is made up of multiple sentences. (not a list of words). Should\n",
    "    be better for hugging face interface.\n",
    "    @param file_path: path to file holding text\n",
    "    @param format: allows function to cover both types of file inputs. \"labeled\" returns source and target lists, while\n",
    "    \"unlabeled\" returns source + root lists and modifiers list of tuples\n",
    "    @param ignored_punctuation: list of what punctuation to leave in sentence\n",
    "    @param source_target_tagging: dictionary for tagging what part of text is target and source\n",
    "    @param root_modifier_tagging: dictionary for tagging what part of text is root and modifiers\n",
    "    @return: depending on label either source + target lists or source + roots + modifier lists\n",
    "    \"\"\"\n",
    "    # read file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # initial cleaning\n",
    "\n",
    "    pattern = '|'.join(sorted(re.escape(obj) for obj in source_target_tagging))\n",
    "    tagged_text = re.sub(pattern, lambda m: source_target_tagging.get(m.group(0)), text, flags=re.IGNORECASE)\n",
    "    if format==\"unlabeled\":\n",
    "        pattern2 = '|'.join(sorted(re.escape(obj) for obj in root_modifier_tagging))\n",
    "        tagged_text = re.sub(pattern2, lambda m: root_modifier_tagging.get(m.group(0)), tagged_text, flags=re.IGNORECASE)\n",
    "        ignored_punctuation += '()'\n",
    "        # ignored_punctuation = ignored_punctuation + '()'\n",
    "    regex_cleaning = dict()\n",
    "    regex_cleaning.update({'\\n': ' '})\n",
    "    regex_cleaning.update({p:'' for p in string.punctuation if p not in ignored_punctuation})\n",
    "    clean_text = tagged_text.translate(str.maketrans(regex_cleaning))\n",
    "    # clean_text = tagged_text\n",
    "    action_items = clean_text.split(\"SOURCE\")\n",
    "\n",
    "    # reorganization\n",
    "    if format == \"labeled\":\n",
    "        source_list = list()\n",
    "        target_list = list()\n",
    "\n",
    "        for action_item in action_items[1:]:\n",
    "            source_target_obj = action_item.split(\"SPLIT\")\n",
    "            source_target_obj = [st_text.strip() for st_text in source_target_obj]\n",
    "            source_list.append(source_target_obj[0])\n",
    "            target_list.append(source_target_obj[1])\n",
    "        return source_list, target_list\n",
    "    elif format == \"unlabeled\":\n",
    "        source_list = list()\n",
    "        root_list = list()\n",
    "        modifier_list = list()\n",
    "        for action_index, action_item in enumerate(action_items[1:]):\n",
    "            source_root_modifier_obj = action_item.split(\"SPLIT\")\n",
    "            source_root_modifier_obj = [st_text.strip() for st_text in source_root_modifier_obj]\n",
    "            source_list.append(source_root_modifier_obj[0].translate(str.maketrans({\"(\": \"\", \")\":\"\"})))\n",
    "            root_list.append(source_root_modifier_obj[1].split(' '))\n",
    "            modifier_tuple_list = list()\n",
    "            modifier_tuples = source_root_modifier_obj[2].translate(str.maketrans({\"(\": \"*\", \")\": \"*\"})).split(\"*\")\n",
    "            for tup_index in range(0, len(modifier_tuples)-1, 2):\n",
    "                modifier_tuple_list.append(tuple(modifier_tuples[tup_index+1].split(' ')))\n",
    "            modifier_list.append(modifier_tuple_list)\n",
    "        return source_list, root_list, modifier_list\n",
    "    else:\n",
    "        raise(\"Error: no process completed\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "def check_file_statistics(filepath):\n",
    "    format = filepath.split('.')[-1]\n",
    "    if format==\"labeled\":\n",
    "        source_list, target_list = clean_text(filepath, format=format)\n",
    "        interest_dict = {\"source_length\": list(), \"target_length\": list()}\n",
    "        for s, t in zip(source_list, target_list):\n",
    "            interest_dict[\"source_length\"].append(len(s.split(' ')))\n",
    "            interest_dict[\"target_length\"].append(len(t.split(' ')))\n",
    "\n",
    "    elif format==\"unlabeled\":\n",
    "        source_list, root_list, modifier_list = clean_text(filepath, format=format)\n",
    "        interest_dict = {\"source_length\": list(), \"root_length\": list(), \"modifier_length\": list()}\n",
    "        for s, r, m in zip(source_list, root_list, modifier_list):\n",
    "            interest_dict[\"source_length\"].append(len(s.split(' ')))\n",
    "            interest_dict[\"root_length\"].append(len(r))\n",
    "            interest_dict[\"modifier_length\"].append(len(m))\n",
    "    else:\n",
    "        raise(\"Does not conform to either either labeled or unlabeled format\")\n",
    "    print(pd.DataFrame.from_dict(interest_dict).describe())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "train_path = 'C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\train.labeled'\n",
    "train_source_list, train_target_list = clean_text(train_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "comp_path = 'C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\comp.unlabeled'\n",
    "comp_source_list, comp_root_list, comp_modifiers_list = clean_text(comp_path, format=\"unlabeled\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  target_length\n",
      "count   10000.000000   10000.000000\n",
      "mean       63.894300      62.265000\n",
      "std        22.171416      21.010681\n",
      "min         1.000000       1.000000\n",
      "25%        49.000000      48.000000\n",
      "50%        63.000000      61.000000\n",
      "75%        78.000000      76.000000\n",
      "max       187.000000     166.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics(train_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  target_length\n",
      "count    1000.000000    1000.000000\n",
      "mean       64.684000      62.890000\n",
      "std        22.068511      20.939206\n",
      "min         1.000000       1.000000\n",
      "25%        50.000000      50.000000\n",
      "50%        63.000000      62.000000\n",
      "75%        79.000000      76.000000\n",
      "max       179.000000     169.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics('C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\val.labeled')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  root_length  modifier_length\n",
      "count    2000.000000  2000.000000      2000.000000\n",
      "mean       64.204500     2.855000         2.855500\n",
      "std        23.265518     1.141327         1.141171\n",
      "min         1.000000     1.000000         1.000000\n",
      "25%        49.000000     2.000000         2.000000\n",
      "50%        63.000000     3.000000         3.000000\n",
      "75%        78.000000     3.000000         3.000000\n",
      "max       262.000000    13.000000        13.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics(comp_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We conclude from here that most sentences are around 50-80 words long.\n",
    "The maximum in the total data set is 262 in the comp file. Which is annoying because it is nearly 100 words longer than the longest in train/val."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
