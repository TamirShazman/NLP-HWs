{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from preprocessing import Preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "default_ip = '!?.'\n",
    "default_st_tagging = {\"German:\\n\": \"SOURCE \", \"English:\\n\": \"SPLIT \"}\n",
    "default_rm_tagging = {\"Roots in English: \": \"SPLIT \", \"Modifiers in English: \": \"SPLIT \"}\n",
    "\n",
    "def clean_text(file_path, format=\"labeled\", ignored_punctuation=default_ip, source_target_tagging=default_st_tagging, root_modifier_tagging=default_rm_tagging):\n",
    "    \"\"\"\n",
    "    Assumes strict order of German than English\n",
    "    Returns two lists, source and target each of which is made up of multiple sentences. (not a list of words). Should\n",
    "    be better for hugging face interface.\n",
    "    @param file_path: path to file holding text\n",
    "    @param format: allows function to cover both types of file inputs. \"labeled\" returns source and target lists, while\n",
    "    \"unlabeled\" returns source + root lists and modifiers list of tuples\n",
    "    @param ignored_punctuation: list of what punctuation to leave in sentence\n",
    "    @param source_target_tagging: dictionary for tagging what part of text is target and source\n",
    "    @param root_modifier_tagging: dictionary for tagging what part of text is root and modifiers\n",
    "    @return: depending on label either source + target lists or source + roots + modifier lists\n",
    "    \"\"\"\n",
    "    # read file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # initial cleaning\n",
    "\n",
    "    pattern = '|'.join(sorted(re.escape(obj) for obj in source_target_tagging))\n",
    "    tagged_text = re.sub(pattern, lambda m: source_target_tagging.get(m.group(0)), text, flags=re.IGNORECASE)\n",
    "    if format==\"unlabeled\":\n",
    "        pattern2 = '|'.join(sorted(re.escape(obj) for obj in root_modifier_tagging))\n",
    "        tagged_text = re.sub(pattern2, lambda m: root_modifier_tagging.get(m.group(0)), tagged_text, flags=re.IGNORECASE)\n",
    "        ignored_punctuation += '()'\n",
    "        # ignored_punctuation = ignored_punctuation + '()'\n",
    "    regex_cleaning = dict()\n",
    "    regex_cleaning.update({'\\n': ' '})\n",
    "    regex_cleaning.update({p:'' for p in string.punctuation if p not in ignored_punctuation})\n",
    "    clean_text = tagged_text.translate(str.maketrans(regex_cleaning))\n",
    "    # clean_text = tagged_text\n",
    "    action_items = clean_text.split(\"SOURCE\")\n",
    "\n",
    "    # reorganization\n",
    "    if format == \"labeled\":\n",
    "        source_list = list()\n",
    "        target_list = list()\n",
    "\n",
    "        for action_item in action_items[1:]:\n",
    "            source_target_obj = action_item.split(\"SPLIT\")\n",
    "            source_target_obj = [st_text.strip() for st_text in source_target_obj]\n",
    "            source_list.append(\"translate German to English: \" + source_target_obj[0])\n",
    "            target_list.append(source_target_obj[1])\n",
    "        return source_list, target_list\n",
    "    elif format == \"unlabeled\":\n",
    "        source_list = list()\n",
    "        root_list = list()\n",
    "        modifier_list = list()\n",
    "        for action_index, action_item in enumerate(action_items[1:]):\n",
    "            source_root_modifier_obj = action_item.split(\"SPLIT\")\n",
    "            source_root_modifier_obj = [st_text.strip() for st_text in source_root_modifier_obj]\n",
    "            source_list.append(\"translate German to English: \" + source_root_modifier_obj[0].translate(str.maketrans({\"(\": \"\", \")\":\"\"})))\n",
    "            root_list.append(source_root_modifier_obj[1].split(' '))\n",
    "            modifier_tuple_list = list()\n",
    "            modifier_tuples = source_root_modifier_obj[2].translate(str.maketrans({\"(\": \"*\", \")\": \"*\"})).split(\"*\")\n",
    "            for tup_index in range(0, len(modifier_tuples)-1, 2):\n",
    "                modifier_tuple_list.append(tuple(modifier_tuples[tup_index+1].split(' ')))\n",
    "            modifier_list.append(modifier_tuple_list)\n",
    "        return source_list, root_list, modifier_list\n",
    "    else:\n",
    "        raise(\"Error: no process completed\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def check_file_statistics(filepath):\n",
    "    \"\"\"\n",
    "    Receives file pathway, loads and cleans data. Then prints statistics about the length of sentences, or root/modifier input\n",
    "    @param filepath:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    format = filepath.split('.')[-1]\n",
    "    if format==\"labeled\":\n",
    "        source_list, target_list = clean_text(filepath, format=format)\n",
    "        interest_dict = {\"source_length\": list(), \"target_length\": list()}\n",
    "        for s, t in zip(source_list, target_list):\n",
    "            interest_dict[\"source_length\"].append(len(s.split(' ')))\n",
    "            interest_dict[\"target_length\"].append(len(t.split(' ')))\n",
    "\n",
    "    elif format==\"unlabeled\":\n",
    "        source_list, root_list, modifier_list = clean_text(filepath, format=format)\n",
    "        interest_dict = {\"source_length\": list(), \"root_length\": list(), \"modifier_length\": list()}\n",
    "        for s, r, m in zip(source_list, root_list, modifier_list):\n",
    "            interest_dict[\"source_length\"].append(len(s.split(' ')))\n",
    "            interest_dict[\"root_length\"].append(len(r))\n",
    "            interest_dict[\"modifier_length\"].append(len(m))\n",
    "    else:\n",
    "        raise(\"Does not conform to either either labeled or unlabeled format\")\n",
    "    print(pd.DataFrame.from_dict(interest_dict).describe())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor('C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\train.labeled', 'C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\val.labeled', tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20721796e615439f9b3390df4db51ff7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06d08e4250a444799facfba0bdd667ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = preprocessor.preprocess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'translate German to English: Was ist da so falsch gelaufen? Die Wirtschaftskrise scheint die naheliegendste ErklÃ¤rung zu sein vielleicht zu naheliegend.',\n 'label': 'What has gone so wrong? The economic crisis seems to be the most obvious explanation but perhaps too obvious.',\n 'input_ids': [13959,\n  2968,\n  12,\n  1566,\n  10,\n  2751,\n  229,\n  836,\n  78,\n  21816,\n  873,\n  8068,\n  58,\n  316,\n  18209,\n  157,\n  7854,\n  18449,\n  67,\n  14462,\n  15342,\n  26,\n  849,\n  28019,\n  170,\n  1110,\n  10330,\n  170,\n  14462,\n  15342,\n  26,\n  5,\n  1],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'labels': [363,\n  65,\n  2767,\n  78,\n  1786,\n  58,\n  37,\n  1456,\n  5362,\n  1330,\n  12,\n  36,\n  8,\n  167,\n  4813,\n  7295,\n  68,\n  2361,\n  396,\n  4813,\n  5,\n  1]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_path = 'C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\train.labeled'\n",
    "train_source_list, train_target_list = clean_text(train_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "comp_path = 'C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\comp.unlabeled'\n",
    "comp_source_list, comp_root_list, comp_modifiers_list = clean_text(comp_path, format=\"unlabeled\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  target_length\n",
      "count   10000.000000   10000.000000\n",
      "mean       63.894300      62.265000\n",
      "std        22.171416      21.010681\n",
      "min         1.000000       1.000000\n",
      "25%        49.000000      48.000000\n",
      "50%        63.000000      61.000000\n",
      "75%        78.000000      76.000000\n",
      "max       187.000000     166.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics(train_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  target_length\n",
      "count    1000.000000    1000.000000\n",
      "mean       64.684000      62.890000\n",
      "std        22.068511      20.939206\n",
      "min         1.000000       1.000000\n",
      "25%        50.000000      50.000000\n",
      "50%        63.000000      62.000000\n",
      "75%        79.000000      76.000000\n",
      "max       179.000000     169.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics('C:\\\\Users\\\\dovid\\\\PycharmProjects\\\\NLP\\\\NLP-HWs\\\\Project\\\\data\\\\val.labeled')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source_length  root_length  modifier_length\n",
      "count    2000.000000  2000.000000      2000.000000\n",
      "mean       64.204500     2.855000         2.855500\n",
      "std        23.265518     1.141327         1.141171\n",
      "min         1.000000     1.000000         1.000000\n",
      "25%        49.000000     2.000000         2.000000\n",
      "50%        63.000000     3.000000         3.000000\n",
      "75%        78.000000     3.000000         3.000000\n",
      "max       262.000000    13.000000        13.000000\n"
     ]
    }
   ],
   "source": [
    "check_file_statistics(comp_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We conclude from here that most sentences are around 50-80 words long.\n",
    "The maximum in the total data set is 262 in the comp file. Which is annoying because it is nearly 100 words longer than the longest in train/val."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create custom dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(pd.DataFrame({'text': train_source_list, 'label': train_target_list}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2383e44b89c241b58e43e244ea38554b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dovid\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    source = [text for text in dataset['text']]\n",
    "    target = [label for label in dataset['label']]\n",
    "    model_inputs = tokenizer(source, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(target, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(train_source_list, text_target=train_target_list, max_length=128, truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d6d3fa1c1a4404e830fa56c50e68ab2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = train_ds.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 10000\n})"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a1395a43749439d864ee0f364c7ad03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dovid\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2f5cd6c74ba4ea0b9511db2cd630700"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dovid\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65a0f46cd10e43b0aba16d3393cbadb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'What has gone so wrong? The economic crisis seems to be the most obvious explanation but perhaps too obvious.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_list[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['translate English to German: Was ist da so falsch gelaufen? Die Wirtschaftskrise scheint die naheliegendste ErklÃ¤rung zu sein vielleicht zu naheliegend.',\n 'translate English to German: Abdullah wollte das Treffen weil er glaubt dass das Weltgeschehen seit dem Jahre 2001 die Bruderschaft der Konservativen gespalten hat. Bis dahin teilten er und Bush eine gemeinsame Weltsicht die die Bedeutung der Religion der traditionellen Familie so wie beider LÃ¤nder sie auffassten gesellschaftliche Disziplin und die Rolle des Staates als UnterstÃ¼tzer dieser Institutionen betonte.',\n 'translate English to German: Hinsichtlich eines absoluten Niedergangs ist zu sagen dass die USA zwar sehr reale Probleme haben aber die amerikanische Wirtschaft dennoch hoch produktiv bleibt. Amerika liegt an erster Stelle bei den Gesamtausgaben fÃ¼r FampE Hochschulrankings Nobelpreisen und auch bei Unternehmensindizes. Laut Angaben des Weltwirtschaftsforums das letzten Monat seinen Bereicht Ã¼ber wirtschaftliche WettbewerbsfÃ¤higkeit verÃ¶ffentlichte liegen die USA auf Platz fÃ¼nf der wettbewerbsfÃ¤higsten Ãkonomien der Welt hinter den kleinen Volkswirtschaften der Schweiz Schwedens Finnlands und Singapurs. China rangiert erst auf Platz 26.',\n 'translate English to German: Ein weiterer Grund fÃ¼r die nachlassende Zustimmung ist die wachsende Ãberzeugung dass dieser Krieg nicht gewonnen werden kann. Peter Feaver von der Duke University ein Experte fÃ¼r Ã¶ffentliche Meinung und gegenwÃ¤rtig Berater des WeiÃen Hauses wies vor kurzem darauf hin dass die Amerikaner Todesopfer nur so lange hinnehmen als sie das GefÃ¼hl haben es handle sich um einen gerechten Krieg mit einigermaÃen intakten Aussichten auf Erfolg. Die amerikanischen BÃ¼rger allerdings bezweifeln beides. Die Administration bezahlt nun den Preis fÃ¼r die Ã¼bertriebene Anpreisung der KriegsgrÃ¼nde und fÃ¼r eine stÃ¼mperhafte Besatzung nach der Invasion. So kommt es nicht Ã¼berraschend dass Bush neuerdings betont eine âStrategie fÃ¼r den Siegâ zu haben.',\n 'translate English to German: In anderen Bereichen stehen wir jedoch mit unseren Ãberlegungen was die Verantwortung von Unternehmen fÃ¼r unser Rechtssystem bedeuten sollte noch ganz am Anfang. Im Zweiten Weltkrieg waren deutsche Unternehmen nur allzu bereit von der Sklavenarbeit der Insassen in Konzentrationslagern zu profitieren und Schweizer Banken freuten sich das Gold jÃ¼discher Opfer des Naziterrors einzusacken. Vor kurzem wurden sie dazu verklagt wenigsten einen Teil dessen zurÃ¼ckzuzahlen was sie genommen hatten.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"translate English to German: \" + input_paragraph for input_paragraph in train_source_list[:5]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "my_pipeline = pipeline(\"translation_de_to_en\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TranslationPipeline' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [41]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmy_pipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslate English to German: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m input_paragraph \u001B[38;5;28;01mfor\u001B[39;00m input_paragraph \u001B[38;5;129;01min\u001B[39;00m train_source_list[\u001B[38;5;241m0\u001B[39m]])\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TranslationPipeline' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "my_pipeline.train([\"translate English to German: \" + input_paragraph for input_paragraph in train_source_list[0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text=[\"translate English to German: \" + input_paragraph for input_paragraph in train_source_list[:5]], max_length=262, truncation=True).input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text=[\"translate English to German: \" + input_paragraph for input_paragraph in train_source_list[:5]], max_length=262, truncation=True, return_tensors=\"pt\").input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def show_input():\n",
    "    yield next(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13959, 1566, 12, 2968, 10, 2751, 229, 836, 78, 21816, 873, 8068, 58, 316, 18209, 157, 7854, 18449, 67, 14462, 15342, 26, 849, 28019, 170, 1110, 10330, 170, 14462, 15342, 26, 5, 1]\n",
      "[13959, 1566, 12, 2968, 10, 28508, 521, 107, 10329, 211, 29008, 5603, 3, 49, 3, 122, 14802, 17, 602, 211, 3779, 30829, 3646, 340, 3861, 4402, 67, 3, 9465, 588, 3445, 74, 2974, 3473, 1528, 29, 3, 2897, 6459, 324, 3, 547, 5, 6483, 22688, 3, 2919, 324, 3, 49, 64, 8905, 266, 6602, 15, 3779, 7, 362, 17, 67, 67, 16144, 74, 18182, 74, 3, 31341, 7453, 78, 587, 5877, 52, 24886, 680, 219, 12837, 324, 3, 25962, 15, 2678, 702, 10574, 64, 67, 11631, 93, 18122, 15, 7, 501, 3941, 29975, 52, 3, 1878, 14932, 35, 3, 27903, 15, 5, 1]\n",
      "[13959, 1566, 12, 2968, 10, 9515, 15690, 266, 7, 6097, 29, 14866, 3810, 7, 229, 170, 10167, 602, 67, 2312, 3, 8297, 1319, 490, 15, 12834, 745, 862, 67, 3, 23384, 20870, 177, 12555, 6012, 3, 21536, 23, 208, 9852, 5, 736, 49, 5561, 5282, 46, 3, 21735, 13524, 468, 177, 13347, 2064, 12525, 218, 377, 4624, 427, 7341, 14800, 6254, 53, 7, 22232, 6837, 35, 64, 319, 468, 13750, 8482, 776, 7, 5, 21523, 3, 17645, 93, 3779, 15119, 7, 24195, 7, 211, 3, 5802, 15071, 3, 6135, 5728, 17, 510, 3, 19594, 3, 19745, 7, 19338, 15261, 15, 12376, 67, 2312, 219, 6140, 9854, 74, 3, 31467, 7, 13718, 1913, 3, 2, 157, 32, 23140, 29, 74, 3779, 7841, 177, 3, 7647, 18982, 15119, 35, 74, 17759, 9430, 537, 7, 22896, 6347, 64, 180, 53, 9, 3791, 7, 5, 1473, 3, 6287, 1378, 5248, 219, 6140, 2208, 5, 1]\n",
      "[13959, 1566, 12, 2968, 10, 890, 3067, 49, 4086, 218, 67, 559, 10764, 221, 1811, 11822, 229, 67, 3, 13253, 221, 3333, 12584, 425, 602, 3, 1878, 15640, 311, 23754, 404, 675, 5, 2737, 4163, 9, 624, 193, 74, 15090, 636, 236, 8865, 15, 218, 18432, 15, 14897, 64, 2995, 7497, 52, 2880, 30294, 93, 20498, 35, 12483, 7, 587, 7, 426, 15012, 51, 6101, 3811, 602, 67, 736, 49, 5561, 687, 304, 1395, 32, 102, 1010, 790, 78, 6575, 3811, 7278, 501, 680, 211, 14037, 745, 3, 15, 7, 2174, 289, 561, 595, 3, 12921, 35, 15640, 181, 4847, 27987, 16, 17, 7935, 35, 27528, 35, 219, 8933, 5, 316, 3, 23384, 29, 11726, 8098, 36, 23075, 4025, 29, 5877, 7, 5, 316, 6863, 28239, 3423, 177, 4670, 218, 67, 510, 13835, 35, 15, 389, 6837, 425, 74, 15640, 7, 12880, 64, 218, 266, 3, 7, 17, 21518, 883, 14786, 493, 5606, 425, 559, 74, 86, 900, 1938, 5, 264, 4287, 3, 15, 7, 311, 28904, 26, 602, 8905, 18601, 26, 53, 7, 3, 27903, 266, 395, 19928, 15, 4044, 218, 177, 15177, 735, 170, 745, 5, 1]\n",
      "[13959, 1566, 12, 2968, 10, 86, 2778, 3, 22136, 5674, 558, 528, 3327, 181, 1324, 29, 3333, 19123, 35, 47, 67, 22282, 193, 4428, 218, 5122, 11013, 3734, 36, 29871, 3402, 763, 1897, 183, 11302, 5, 1318, 11280, 324, 31131, 3758, 3, 11751, 4428, 790, 66, 1000, 10313, 193, 74, 180, 8142, 1926, 5269, 74, 86, 7, 9, 4932, 16, 2974, 1847, 17, 2661, 7, 13298, 29, 170, 25641, 64, 22446, 1925, 35, 24537, 35, 289, 211, 2540, 3, 354, 1272, 26, 8421, 25209, 93, 17562, 17, 17262, 7, 13575, 15525, 35, 5, 1526, 15012, 51, 3163, 680, 4042, 548, 157, 5430, 17, 62, 31103, 595, 3560, 3, 10713, 4204, 1000, 10814, 47, 680, 3, 7026, 8827, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in input_ids:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class full_paragraph_T5(Pipeline):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__(self)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def _sanitize_parameters(self, **pipeline_parameters):\n",
    "        preprocess_kwargs = dict()\n",
    "        if \"max_length\" in pipeline_parameters:\n",
    "            preprocess_kwargs[\"max_length\"] = pipeline_parameters[\"max_length\"]\n",
    "        if \"truncation\" in pipeline_parameters:\n",
    "            preprocess_kwargs[\"truncation\"] = pipeline_parameters[\"truncation\"]\n",
    "        return preprocess_kwargs, dict(), dict()\n",
    "    def preprocess(self, input_: Any, **preprocess_parameters: Dict) -> Dict[str, GenericTensor]:\n",
    "        model_input = self.tokenizer(input_)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
